---
title: "Week 10 Study Notes: Logistic Regression for Binary Outcomes"
author: "MUSA 5080"
date: "November 10, 2025"
format: 
  html:
    toc: true
    toc-depth: 3
    theme: cosmo
---

# Part 1: Understanding Binary Outcomes

## When Do We Use Logistic Regression?

Logistic regression is perfect for binary classification problems in policy:

### Criminal Justice
- Will someone reoffend? (recidivism)
- Will someone appear for court? (flight risk)

### Health
- Will patient develop disease? (risk assessment)
- Will treatment be successful? (outcome prediction)

### Economics
- Will loan default? (credit risk)
- Will person get hired? (employment prediction)

### Urban Planning
- Will building be demolished? (blight prediction)
- Will household participate in program? (uptake prediction)

---

## Why Linear Regression Doesn't Work

### Problems with Linear Regression for Binary Outcomes

**The fundamental issue**: Linear regression can predict values outside [0, 1]

1. **Predictions can be > 1 or < 0** (makes no sense for probability!)
2. **Assumes constant effect across range** (not realistic)
3. **Violates regression assumptions** (errors aren't normal)

### Example: Predicting Exam Pass/Fail

If we use linear regression to predict whether a student passes (1) or fails (0) based on study hours:

- For high study hours: might predict 1.3 (impossible - can't be more than 100% likely)
- For low study hours: might predict -0.2 (impossible - can't be negative probability)

**Linear regression treats outcomes as continuous, but binary outcomes are fundamentally different.**

---

# Part 2: The Logistic Function

## The Solution: Transform the Problem

Instead of predicting Y directly, predict the **probability** that Y = 1.

### The Logistic Function

$$p(X) = \frac{1}{1+e^{-(\beta_0 + \beta_1X_1 + ... + \beta_kX_k)}}$$

**Key properties**:
- Always outputs values between 0 and 1
- S-shaped curve
- Smooth transition from 0 to 1

### Why It Works

- When $\beta_0 + \beta_1X_1 + ... = 0$: probability = 0.5
- When very positive: probability approaches 1
- When very negative: probability approaches 0
- **Never exceeds 1 or goes below 0**

---

## The Logit Transformation

### Working with Log-Odds

Behind the scenes, we work with **log-odds**, not probabilities directly.

**Odds**: 
$$\text{Odds} = \frac{p}{1-p}$$

**Log-Odds (Logit)**: 
$$\text{logit}(p) = \ln\left(\frac{p}{1-p}\right)$$

This creates a **linear relationship**:
$$\ln\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1X_1 + \beta_2X_2 + ...$$

### Why This Matters

- Coefficients represent changes in **log-odds** (like linear regression!)
- We exponentiate coefficients to get **odds ratios**: $e^{\beta}$
- **OR > 1**: predictor increases odds of outcome
- **OR < 1**: predictor decreases odds of outcome
- **OR = 1**: no effect

---

# Part 3: Interpreting Logistic Regression

## Understanding Coefficients

### Raw Coefficients (Log-Odds Scale)

**Example**: Predicting email spam

| Coefficient | Value | Interpretation |
|-------------|-------|----------------|
| Intercept | -4.2 | Log-odds when all predictors = 0 |
| exclamation_marks | 0.45 | Each ! increases log-odds by 0.45 |
| contains_free | 2.1 | Having "free" increases log-odds by 2.1 |
| length | -0.003 | Each character decreases log-odds by 0.003 |

**Problem**: Log-odds are hard to interpret intuitively!

### Odds Ratios (Exponentiated Coefficients)

**Convert to odds ratios**: $OR = e^{\beta}$

| Variable | Coefficient | Odds Ratio | Interpretation |
|----------|-------------|------------|----------------|
| exclamation_marks | 0.45 | 1.57 | Each ! multiplies odds by 1.57 (57% increase) |
| contains_free | 2.1 | 8.17 | "Free" multiplies odds by 8.17 |
| length | -0.003 | 0.997 | Each character multiplies odds by 0.997 |

### Converting to Probability

**From odds to probability**:
$$p = \frac{\text{odds}}{1 + \text{odds}}$$

**Example**: If odds = 3, then $p = \frac{3}{1+3} = 0.75$ (75% probability)

---

# Part 4: Making Predictions

## From Probabilities to Decisions

**The logistic regression model outputs probabilities between 0 and 1.**

But policy decisions require **binary choices**: spam/not spam, approve/deny, intervene/don't intervene.

### The Threshold Decision

**Key question**: What probability triggers action?

- **Threshold = 0.5** (common default): Predict "yes" if p > 0.5
- **Threshold = 0.3** (more aggressive): Predict "yes" if p > 0.3
- **Threshold = 0.7** (more conservative): Predict "yes" if p > 0.7

### Example: Email Spam Filter

If predicted probability of spam = 0.62:

- Threshold = 0.5 → **Classify as spam**
- Threshold = 0.7 → **Classify as NOT spam**

**There is no universal "right" threshold - it depends on the costs of each type of error.**

---

# Part 5: Evaluating Binary Predictions

## The Confusion Matrix

When we make binary predictions, four outcomes are possible:

### The Four Outcomes

|  | **Actual: Positive** | **Actual: Negative** |
|---|---|---|
| **Predicted: Positive** | True Positive (TP) ✓ | False Positive (FP) ✗ |
| **Predicted: Negative** | False Negative (FN) ✗ | True Negative (TN) ✓ |

### Understanding Each Cell

- **True Positive (TP)**: Correctly predicted positive
- **True Negative (TN)**: Correctly predicted negative
- **False Positive (FP)**: Incorrectly predicted positive (Type I error)
- **False Negative (FN)**: Incorrectly predicted negative (Type II error)

---

## Performance Metrics

### Sensitivity (Recall, True Positive Rate)

$$\text{Sensitivity} = \frac{TP}{TP + FN}$$

**Interpretation**: Of all actual positives, how many did we catch?

**Example**: Of 100 actual spam emails, we correctly identified 85
- Sensitivity = 85/100 = 0.85 (85%)

### Specificity (True Negative Rate)

$$\text{Specificity} = \frac{TN}{TN + FP}$$

**Interpretation**: Of all actual negatives, how many did we correctly identify?

**Example**: Of 900 legitimate emails, we correctly identified 850
- Specificity = 850/900 = 0.944 (94.4%)

### Precision (Positive Predictive Value)

$$\text{Precision} = \frac{TP}{TP + FP}$$

**Interpretation**: Of all our positive predictions, how many were correct?

**Example**: We flagged 135 emails as spam, 85 actually were
- Precision = 85/135 = 0.630 (63%)

### Accuracy

$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$$

**Warning**: Accuracy can be misleading with imbalanced data!


# Part 6: The Threshold Trade-off

## The Sensitivity-Specificity Trade-off

**As you increase the threshold**:
- ↓ Sensitivity (catch fewer true positives)
- ↑ Specificity (fewer false alarms)
- ↓ False Positive Rate
- ↑ False Negative Rate

**As you decrease the threshold**:
- ↑ Sensitivity (catch more true positives)
- ↓ Specificity (more false alarms)
- ↑ False Positive Rate
- ↓ False Negative Rate

### Visual Understanding

**Threshold = 0.3** (Low bar - flag more as positive)
```
More positives predicted
├─ More true positives (↑ sensitivity) ✓
└─ More false positives (↓ specificity) ✗
```

**Threshold = 0.7** (High bar - flag fewer as positive)
```
Fewer positives predicted
├─ Fewer false positives (↑ specificity) ✓
└─ Fewer true positives (↓ sensitivity) ✗
```

---

## Two Policy Scenarios

### Scenario A: Rare, Deadly Disease Screening

**Context**:
- Disease is rare but fatal if untreated
- Treatment is safe with minor side effects

**Goal**: Don't miss any cases (high sensitivity)

**Acceptable**: Some false positives (unnecessary treatment)

**Threshold choice**: **Low** (e.g., 0.2)
- Catch all cases, even at risk of false alarms
- Better to treat unnecessarily than miss a case

### Scenario B: High-Risk Individual Identification

**Context**:
- Limited intervention slots
- False positives waste resources
- False negatives miss opportunities to help

**Goal**: Use resources efficiently (high precision)

**Threshold choice**: **High** (e.g., 0.7)
- Only intervene when very confident
- Depends on: Cost of intervention vs. cost of missed case

---

# Part 7: ROC Curves

## What Is an ROC Curve?

**ROC = Receiver Operating Characteristic**

Originally developed for radar signal detection in WWII.

### What It Shows

- **Every possible threshold** in one visualization
- Trade-off between True Positive Rate (Sensitivity) and False Positive Rate (1 - Specificity)
- Overall model discrimination ability

### How to Read It

- **X-axis**: False Positive Rate = 1 - Specificity
- **Y-axis**: True Positive Rate = Sensitivity
- **Diagonal line**: Random guessing (AUC = 0.5)
- **Top-left corner**: Perfect prediction (AUC = 1.0)
- **Curve above diagonal**: Better than random
- **Curve below diagonal**: Worse than random (model is backwards!)

---

## Area Under the Curve (AUC)

### Interpreting AUC

**AUC summarizes overall model performance**:

| AUC Range | Interpretation |
|-----------|----------------|
| **1.0** | Perfect classifier |
| **0.9 - 1.0** | Excellent |
| **0.8 - 0.9** | Good |
| **0.7 - 0.8** | Acceptable |
| **0.6 - 0.7** | Poor |
| **0.5** | No better than random guessing |
| **< 0.5** | Worse than random (model backwards!) |

### What AUC Tells You

**AUC = 0.85 means**:
- If you pick a random positive case and a random negative case
- 85% of the time, the model assigns higher probability to the positive case

### What AUC Doesn't Tell You

1. **Which threshold to use** (you still need to choose!)
2. **Performance with class imbalance** (can be misleading)
3. **Equity implications** (groups may have different AUCs)
4. **Cost of different errors** (treats FP and FN equally)

---

## Key Points on ROC Curves

### The Shape Tells a Story

**Curve hugs top-left**: Excellent separation between classes
- High sensitivity AND high specificity possible
- Can choose threshold with minimal trade-off

**Curve near diagonal**: Poor separation
- No threshold gives good performance
- Model barely better than guessing

**Curve has steep initial rise**: Good for high-sensitivity applications
- Can achieve high sensitivity with low FP rate

---

# Part 8: Equity Considerations

## The Core Problem: Disparate Impact

**A model can be "accurate" overall but perform very differently across groups.**

### Example: Recidivism Model

| Group | Sensitivity | Specificity | False Positive Rate |
|-------|-------------|-------------|---------------------|
| Overall | 0.72 | 0.68 | 0.32 |
| Group A | 0.78 | 0.74 | 0.26 |
| Group B | 0.64 | 0.58 | 0.42 |

**Group B experiences**:
- Lower sensitivity (more people who will reoffend are missed)
- Lower specificity (more people who won't reoffend are flagged)
- **Higher false positive rate (more unjust interventions)**

**This is algorithmic bias in action.**

---

## Case Study: COMPAS

**COMPAS**: Commercial algorithm used in criminal justice to predict recidivism

### ProPublica Investigation (2016)

**Findings**:
- Similar overall accuracy for Black and White defendants
- **BUT**: False positive rates differed dramatically
  - Black defendants: **45% false positive rate**
  - White defendants: **23% false positive rate**
- Black defendants **twice as likely** to be incorrectly labeled "high risk"

### The Implications

**For Black defendants**:
- More likely to be denied bail
- Longer sentences
- More restrictive parole conditions
- **Based on incorrect predictions**

**Key insight**: Overall accuracy masks disparate impact

### The Fairness Impossibility Theorem

**Different definitions of fairness can't all be satisfied simultaneously**:

1. **Calibration**: Same predicted probability means same actual risk across groups
2. **Equalized odds**: Same true/false positive rates across groups
3. **Predictive parity**: Same precision across groups

**You can't have all three!** Must choose which type of fairness matters most for your context.

---

## Testing for Disparate Impact

### What to Check

1. **Overall performance by group**:
   - AUC by demographic group
   - Accuracy by group

2. **Error rates by group**:
   - False Positive Rate (FPR)
   - False Negative Rate (FNR)
   - Is one group experiencing more errors?

3. **Threshold impact**:
   - Same threshold affects groups differently
   - Consider group-specific thresholds?

4. **Base rates**:
   - Are outcome rates different across groups?
   - This affects what "fair" means

### Red Flags

- Large differences in FPR across groups (disparate impact)
- One group has much lower sensitivity (missed opportunities)
- One group has much lower specificity (unjust interventions)
- AUC differs substantially by group (model works differently)

---

# Part 9: Choosing a Threshold

## Framework for Threshold Selection

### Step 1: Understand the Consequences

**Ask**:
- What happens with a false positive?
- What happens with a false negative?
- Are costs symmetric or asymmetric?

**Example: Criminal bail decision**
- False Positive: Deny bail to someone who would appear
  - Cost: Unjust detention, lost income, family disruption
- False Negative: Grant bail to someone who won't appear
  - Cost: Missed court date, resources to locate

### Step 2: Consider Stakeholder Perspectives

**Ask**:
- Who is affected by each type of error?
- Do all groups experience consequences equally?
- What are power dynamics?

**Example: Loan approval**
- False Positive (deny good borrower):
  - Individual: Can't buy home, build wealth
  - Bank: Missed profit opportunity
- False Negative (approve bad borrower):
  - Individual: Debt they can't repay
  - Bank: Financial loss

### Step 3: Choose Your Metric Priority

**Options**:
1. **Maximize sensitivity**: Catch all positives (minimize FN)
2. **Maximize specificity**: Minimize false alarms (minimize FP)
3. **Balance precision and recall**: F1 score
4. **Equalize across groups**: Fairness-aware thresholds
5. **Minimize expected cost**: Cost-benefit analysis

### Step 4: Test Multiple Thresholds

**Process**:
1. Evaluate performance across range of thresholds (0.1 to 0.9)
2. Look at group-wise performance at each threshold
3. Consider sensitivity analysis (how robust is choice?)
4. Document trade-offs explicitly

---

## Cost-Benefit Analysis Approach

### Assigning Concrete Costs

**Example: Disease screening**

| Outcome | Cost | Benefit |
|---------|------|---------|
| True Positive | Treatment: $1,000 | Prevent complications: $50,000 |
| False Positive | Unnecessary treatment: $1,000 | None |
| True Negative | None | None |
| False Negative | Miss disease | Later complications: $50,000 |

### Calculate Expected Cost

$$E[\text{Cost}] = C_{FP} \times FP + C_{FN} \times FN - B_{TP} \times TP$$

**Choose threshold that minimizes expected cost.**

### Limitations

- Assumes we can quantify all costs (often impossible!)
- Doesn't capture justice/equity concerns
- Who decides what costs matter?
- Some harms can't be monetized (dignity, autonomy, trust)

---

## Practical Recommendations

### Best Practices

1. **Report multiple metrics** - not just accuracy
   - Sensitivity, specificity, precision, F1
   - Show confusion matrix
   - Report by sub-group

2. **Show the ROC curve** - demonstrates trade-offs
   - Makes threshold choice explicit
   - Shows range of possible performance

3. **Test multiple thresholds** - document your choice
   - Why did you choose this threshold?
   - What trade-offs did you accept?

4. **Evaluate by sub-group** - check for disparate impact
   - Performance metrics by demographic group
   - Error rates by group
   - Consider group-specific thresholds

5. **Document assumptions** - explain your reasoning
   - What costs did you consider?
   - Who did you consult?
   - What alternatives did you consider?

6. **Consider context** - real-world consequences matter
   - What happens after prediction?
   - Can decisions be appealed?
   - Is there human oversight?

7. **Provide uncertainty** - confidence intervals
   - Not just point estimates
   - Show prediction intervals
   - Acknowledge limitations

8. **Enable recourse** - can predictions be challenged?
   - How can someone contest a prediction?
   - What information is provided?
   - Is the process transparent?

---

# Part 10: Model Implementation

## Fitting Logistic Regression

### The Basic Model

**Formula**:
```
outcome ~ predictor1 + predictor2 + predictor3
```

**Family**: `binomial` (specifies logistic regression)

**Link**: `logit` (default for binomial)

### Making Predictions

**Type options**:
- `type = "link"`: Returns log-odds (linear predictor)
- `type = "response"`: Returns probabilities (what you usually want)

### Example Interpretation

**Model**: Predicting loan default

| Variable | Coefficient | Odds Ratio | Interpretation |
|----------|-------------|------------|----------------|
| (Intercept) | -5.2 | 0.006 | Very low baseline odds |
| Income (per $1000) | -0.15 | 0.86 | Each $1000 income → 14% lower odds |
| Credit score | -0.02 | 0.98 | Each point → 2% lower odds |
| Prior defaults | 1.5 | 4.48 | Prior default → 348% higher odds |

---

## Common Pitfalls

### 1. Class Imbalance

**Problem**: When one outcome is rare (e.g., 95% negative, 5% positive)
- Model can achieve 95% accuracy by always predicting negative!
- Sensitivity may be 0% (catches no positives)

**Solutions**:
- Don't use accuracy as main metric
- Focus on sensitivity/specificity/F1
- Consider resampling (SMOTE, undersampling)
- Adjust decision threshold

### 2. Perfect Separation

**Problem**: When a predictor perfectly predicts outcome
- Coefficients become infinite
- Model fails to converge

**Example**: Everyone with credit score < 500 defaults, everyone ≥ 500 doesn't

**Solutions**:
- Check for perfect predictors
- Add regularization (ridge/lasso)
- Remove problematic predictors

### 3. Multicollinearity

**Problem**: Predictors highly correlated with each other
- Unstable coefficients
- Hard to interpret individual effects

**Example**: Including both income and home value (highly correlated)

**Solutions**:
- Check correlation matrix
- Remove redundant predictors
- Use principal components

### 4. Overfitting

**Problem**: Model learns training data too well, doesn't generalize
- High training accuracy, poor test accuracy
- Complex models with many predictors

**Solutions**:
- Use cross-validation
- Regularization (ridge/lasso)
- Simplify model
- Get more training data

---

# Key Takeaways

## Core Concepts

1. **Logistic regression predicts probabilities**, not outcomes directly
   - Outputs are always between 0 and 1
   - Uses logit transformation for linearity

2. **Interpreting coefficients requires care**
   - Raw coefficients are log-odds (hard to interpret)
   - Odds ratios (exp(β)) are more intuitive
   - OR > 1 means positive relationship, OR < 1 means negative

3. **Threshold choice is critical and subjective**
   - No universal "right" threshold
   - Depends on costs of false positives vs. false negatives
   - Makes trade-offs explicit

4. **Multiple metrics matter**
   - Accuracy alone is insufficient (especially with imbalance)
   - Sensitivity, specificity, precision tell different stories
   - ROC curve shows all thresholds at once

5. **Equity requires active checking**
   - Overall accuracy can mask disparate impact
   - Error rates may differ across groups
   - Must explicitly test for bias

