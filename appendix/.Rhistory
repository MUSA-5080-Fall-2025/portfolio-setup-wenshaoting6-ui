labs(title = "Distribution of Home Sale Prices by Number of Bedrooms",
subtitle = "For Philadelphia Residential Properties in 2023-2024",
x = "Bedrooms",
y = "Sale Price") +
theme_minimal() +
theme(
plot.title = element_text(face = "bold")
)
ggplot(phl_sales_clean, aes(x = factor(number_of_bedrooms), y = log(sale_price))) +
geom_boxplot(fill = "lightcyan2", outlier.color = "firebrick", outlier.alpha = 0.2) +
scale_y_continuous(labels = scales::label_dollar()) +
labs(title = "Distribution of Log(Home Sale Prices) by Number of Bedrooms",
subtitle = "For Philadelphia Residential Properties in 2023-2024",
x = "Bedrooms",
y = "Log(Sale Price)") +
theme_minimal() +
theme(
plot.title = element_text(face = "bold")
)
ggplot(phl_sales_clean, aes(x = total_livable_area, sale_price)) +
geom_point(alpha = 0.1, size = 1) +
geom_smooth(method = "lm", se = FALSE, linetype = 5, color = "firebrick") +
scale_y_continuous(labels = label_dollar()) +
labs(title = "Distribution of Home Sale Prices by Total Livable Area",
subtitle = "For Philadelphia Residential Properties in 2023-2024",
x = "Total Livable Area",
y = "Sale Price") +
theme_minimal() +
theme(
plot.title = element_text(face = "bold")
)
ggplot(phl_sales_clean, aes(x = log(total_livable_area), log(sale_price))) +
geom_point(alpha = 0.075, size = 1) +
geom_smooth(method = "lm", se = FALSE, linetype = 5, color = "firebrick") +
scale_y_continuous(labels = label_dollar()) +
labs(title = "Distribution of Log(Home Sale Prices) by Total Livable Area",
subtitle = "For Philadelphia Residential Properties in 2023-2024",
x = "Log(Total Livable Area)",
y = "Log(Sale Price)") +
theme_minimal() +
theme(
plot.title = element_text(face = "bold")
)
ggplot(phl_sales_clean, aes(x = age, y = sale_price)) +
geom_point(alpha = 0.076, size = 1) +
scale_y_continuous(labels = label_dollar()) +
labs(title = "Distribution of Home Sale Prices by Age",
subtitle = "For Philadelphia Residential Properties in 2023-2024",
x = "Age",
y = "Sale Price") +
theme_minimal() +
theme(
plot.title = element_text(face = "bold")
)
ggplot(phl_sales_clean, aes(x = age, log(sale_price))) +
geom_point(alpha = 0.075, size = 1) +
geom_smooth(method = "loess", span = 0.75, se = FALSE, color = "firebrick") +
scale_y_continuous(labels = label_dollar()) +
labs(title = "Distribution of Log(Home Sale Prices) by Age",
subtitle = "For Philadelphia Residential Properties in 2023-2024",
x = "Age",
y = "Log(Sale Price)") +
theme_minimal() +
theme(
plot.title = element_text(face = "bold")
)
df_age_groups <- phl_sales_clean |>
mutate(
age_group = case_when(
age < 20 ~ "New (<20 years)",
age < 80 ~ "Middle (20–80 years)",
age >= 80 ~ "Historic (>80 years)"
),
age_group = factor(age_group, levels = c("New (<20 years)", "Middle (20–80 years)", "Historic (>80 years)"))
)
ggplot(df_age_groups, aes(age_group, log(sale_price))) +
geom_boxplot(fill = "lightcyan2", outlier.colour = "firebrick", outlier.alpha = 0.2) +
labs(
title = "Distribution of Log(Sale Price) by Age Group",
subtitle = "Philadelphia Residential Sales in 2023–2024",
x = "Age Group", y = "log(Sale Price)",
) +
theme_minimal() +
theme(
plot.title = element_text(face = "bold")
)
ggplot(phl_sales_clean_sf_final, aes(x = EconKDE$lyr.1, y=sale_price)) +
geom_point(alpha = 0.1, size = 1) +
geom_smooth(method = "lm", se = FALSE, linetype = 5, color = "firebrick") +
scale_y_continuous(labels = label_dollar()) +
labs(title = "Distribution of Home Sale Prices by Economic KDE Value",
subtitle = "For Philadelphia Residential Properties in 2023-2024",
x = "KDE",
y = "Sale Price") +
theme_minimal() +
theme(
plot.title = element_text(face = "bold")
)
# 99th percentile of sales prices to remove
price_99_perc <- quantile(phl_sales_clean$sale_price, 0.99, na.rm = TRUE)
# Relevant columns to keep for modeling
rel_columns <- c(
# Location / Shape
"census_tract", "shape", "location", "zip_code",
# Target
"sale_price", "log_price",
# Regressors (and potential ones)
"total_livable_area", "log_total_livable_area",
"number_of_bedrooms", "number_of_bathrooms",
"year_built", "age", "age_group",
"interior_condition", "quality_grade"
)
phl_sales_final <- phl_sales_clean |>
filter(
# Remove top 1% of sale price (extreme outliers)
# This creates an upper bound of around $2 million rather than $6 million to further reduce outlier effects in modeling
sale_price < price_99_perc,
# Remove bathrooms > 7
number_of_bathrooms < 8,
# Remove bedrooms > 9
number_of_bedrooms < 10
)|>
mutate(
# Log of sale price
log_price = log(sale_price),
# Log of total livable area
log_total_livable_area = log(total_livable_area),
# Age group buckets (new, middle_age, historic)
age_group = case_when(
age < 20 ~ "New (<20)",
age <= 80 ~ "Middle (20–80)",
age > 80 ~ "Historic (>80)"
),
# For modeling as dummy variables
age_group = factor(age_group, levels = c("New (<20)", "Middle (20–80)", "Historic (>80)"))
) |>
# Select relevant columns
select(any_of(rel_columns))
#convert housing prices data into point data
phl_sales_final_sf = phl_sales_final%>%
mutate(geometry = st_as_sfc(shape)) %>%   # parse WKT into geometry
st_as_sf(crs = 2272)
#convert and match the crs
philly_schools_sf_final=philly_schools_sf_clean%>%
st_transform(2272)
philly_neighborhoods=philly_neighborhoods%>%
st_transform(2272)
philly_tract_map=philly_tract_map%>%
st_transform(2272)
#merge them together
phl_sales_final_sf_final=phl_sales_final_sf%>%
st_join(philly_tract_map,join=st_within)%>%
st_join(philly_neighborhoods,join = st_within)
phl_sales_final_sf_final <- st_transform(phl_sales_final_sf_final, crs = st_crs(r_trees))
phl_sales_final_sf_final$EconKDE <- raster::extract(r_Economic,phl_sales_final_sf_final)
phl_sales_final_sf_final$TreeKDE <- raster::extract(r_trees, phl_sales_final_sf_final)
phl_sales_final_sf_final=phl_sales_final_sf_final%>%
st_transform(2272)
#convert census tract into points
census_points=st_centroid(philly_tract_map)
points_joined <- st_join(census_points, philly_neighborhoods)
# Calculate mean MHI for each polygon
mean_mhi_by_poly <- points_joined %>%
st_drop_geometry() %>%
group_by(MAPNAME) %>%
summarise(meanMHI = mean(median_income, na.rm = TRUE))
# Join the result back to the polygon layer
phl_sales_final_sf_final <- phl_sales_final_sf_final %>%
left_join(mean_mhi_by_poly, by = "MAPNAME")%>%
st_as_sf()
# reclassify the neighborhood data based on quantile (25%)
phl_sales_final_sf_final$MHI_quantile <- cut(
phl_sales_final_sf_final$meanMHI,
breaks = quantile(
phl_sales_final_sf_final$meanMHI,
probs = seq(0, 1, 0.25),
na.rm = TRUE
),
include.lowest = TRUE,
labels = c("Q1 (lowest)", "Q2", "Q3", "Q4 (highest)")
)
# Compute distance matrix
dist_matrix <- st_distance(phl_sales_final_sf_final, philly_schools_sf_final)
# Extract the 3 smallest distances for each point
nearest_3 <- apply(dist_matrix, 1, function(x) sort(x)[1:3])
# Get the average or total distance if needed
mean_nearest3 <- apply(dist_matrix, 1, function(x) mean(sort(x)[1:3]))
phl_sales_final_sf_final$mean_3nn_dist <- mean_nearest3
#reformat raster data into a "readable" format by stargazer
phl_sales_final_sf_final <- phl_sales_final_sf_final %>%
mutate(EconKDE = EconKDE$lyr.1,
TreeKDE = TreeKDE$lyr.1)
#exclude the unnecessary data
summary_data <- phl_sales_final_sf_final %>%
st_drop_geometry() %>%
select(-c(
census_tract, shape, NAME.x, NAME.y, MAPNAME,
Shape_Area, Shape_Leng, location, zip_code, GEOID,
variable, age_group, quality_grade, LISTNAME, MHI_quantile,
estimate,moe
))
# Keep only numeric columns
summary_numeric <- summary_data %>%
select(where(is.numeric))
summary_numeric_df <- as.data.frame(summary_numeric)
# Save as HTML locally
stargazer(summary_numeric_df,
type = "html",
title = "Summary Statistics",
digits = 2,
summary.stat = c("n", "mean", "sd", "min", "median", "max"),
covariate.labels = c("Sale Price", "Log Price", "Total Livable Area",
"Log Total Livable Area", "Bedrooms", "Bathrooms",
"Year Built", "Age", "Interior Condition",
"Median Income","Family HH",
"Total HH", "% Bachelors", "% Vacant", "% White",
"Econ KDE", "Tree KDE", "Mean MHI",
"Mean 3 closest School Dist"),
out = "summary_statistics.html")
# Print as txt in R
stargazer(summary_numeric_df,
type = "text",
title = "Summary Statistics",
digits = 2,
summary.stat = c("n", "mean", "sd", "min", "median", "max"),
covariate.labels = c("Sale Price", "Log Price", "Total Livable Area",
"Log Total Livable Area", "Bedrooms", "Bathrooms",
"Year Built", "Age", "Interior Condition",
"Median Income","Family HH",
"Total HH", "% Bachelors", "% Vacant", "% White",
"Econ KDE", "Tree KDE", "Mean MHI",
"Mean 3 closest School Dist"))
for_model_building <- phl_sales_final_sf_final %>%
st_drop_geometry() %>%
select(-c(
census_tract, shape, NAME.x, NAME.y,
Shape_Area, Shape_Leng, location, zip_code, GEOID,
variable, LISTNAME,
estimate,moe
))
set.seed(123)
n <- nrow(for_model_building)
# 70% training, 30% testing
train_indices <- sample(1:n, size = 0.7 * n)
train_data <- for_model_building[train_indices, ]
test_data <- for_model_building[-train_indices, ]
#run regression based on houses own characteristic
options(scipen = 999)
model1=lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
+interior_condition+quality_grade,data=for_model_building)
summary(model1)
#check colinearity
vif(model1)
#vif looks fine, check heteroskedasticity
bptest(model1)
#p-value is way too small, implying more variables may be needed (make sense)
#calculate the RMSE
model_train_1 <- lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age+interior_condition+quality_grade,data = train_data)
test_predictions_1 <- predict(model_train_1, newdata = test_data)
rmse_test_1 <- sqrt(mean((test_data$log_price - test_predictions_1)^2,na.rm = TRUE))
rmse_train_1 <- summary(model_train_1)$sigma
plot(model1)
options(scipen = 999)
model2=lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
+interior_condition+median_income+I(family_hh/total_hh)
+pct_bachelors+pct_vacant+pct_white+quality_grade ,data=for_model_building)
summary(model2)
#number of bedroom becomes statistical insignificant after controling census variable
#check colinearity
vif(model2)
#edu_attainment and mhi may have muticolinearity
bptest(model2)
#p-value is way too small, implying more variables may be needed (make sense)
#calculate the RMSE
model_train_2 <- lm(log_price~log_total_livable_area+number_of_bedrooms
+number_of_bathrooms+age+interior_condition+median_income
+I(family_hh/total_hh)
+pct_bachelors+pct_vacant+pct_white
+quality_grade,data = train_data)
test_predictions_2 <- predict(model_train_2, newdata = test_data)
rmse_test_2 <- sqrt(mean((test_data$log_price - test_predictions_2)^2,na.rm = TRUE))
rmse_train_2 <- summary(model_train_2)$sigma
plot(model2)
options(scipen = 999)
model3=lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
+interior_condition+median_income+I(family_hh/total_hh)
+pct_bachelors+pct_vacant+pct_white+EconKDE+I(EconKDE^2)+TreeKDE
+mean_3nn_dist+quality_grade,data=for_model_building)
summary(model3)
#percent of family hh become not significant, number of bedrooms become significant again
#check colinearity
vif(model3)
#very sure edu_attainment and mhi may have muticolinearity, make sense EconKDE and its squared term have high vif score as we are taking quadratic term
bptest(model3)
#p-value is way too small, implying more variables may be needed
#calculate the RMSE
model_train_3 <- lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
+interior_condition+median_income+I(family_hh/total_hh)
+pct_bachelors+pct_vacant+pct_white+EconKDE+I(EconKDE^2)+TreeKDE
+mean_3nn_dist+quality_grade,data = train_data)
test_predictions_3 <- predict(model_train_3, newdata = test_data)
rmse_test_3 <- sqrt(mean((test_data$log_price - test_predictions_3)^2,na.rm = TRUE))
rmse_train_3 <- summary(model_train_3)$sigma
plot(model3)
options(scipen = 999)
model4=lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
+interior_condition+median_income+I(family_hh/total_hh)
+pct_bachelors+pct_vacant+pct_white+EconKDE+I(EconKDE^2)+TreeKDE
+mean_3nn_dist+MHI_quantile*age,data=for_model_building)
summary(model4)
#percent of family hh become not significant, number of bedrooms become significant again
#check colinearity
vif(model4)
#very sure edu_attainment and mhi may have muticolinearity
#tree seems to be correlated with neighborhood (make sense)
#make sense EconKDE and its squared term have high vif score as we are taking quadratic term
bptest(model4)
#p-value is way too small, implying more variables may be needed
#calculate the RMSE
model_train_4 <- lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
+interior_condition+median_income+I(family_hh/total_hh)
+pct_bachelors+pct_vacant+pct_white+EconKDE+I(EconKDE^2)+TreeKDE
+mean_3nn_dist+MHI_quantile*age,data = train_data)
test_predictions_4 <- predict(model_train_4, newdata = test_data)
rmse_test_4 <- sqrt(mean((test_data$log_price - test_predictions_4)^2,na.rm = TRUE))
rmse_train_4 <- summary(model_train_4)$sigma
plot(model4)
rmse_values = c(rmse_test_1,rmse_test_2,rmse_test_3,rmse_test_4)
adj_r2 = c(
summary(model1)$adj.r.squared,
summary(model2)$adj.r.squared,
summary(model3)$adj.r.squared,
summary(model4)$adj.r.squared
)
model_summary <- data.frame(
Model = c("Model 1", "Model 2", "Model 3", "Model 4"),
RMSE = round(rmse_values, 3),
Adj_R2 = round(adj_r2, 3)
)
stargazer(model_summary,
type = "html",
title = "Model Performance Summary",
summary = FALSE,
rownames = FALSE,
out = "model_performance_summary.html")
stargazer(model_summary,
type = "text",
title = "Model Performance Summary",
summary = FALSE,
rownames = FALSE)
stargazer(model1,model2,model3,model4,
type = "html", #could be changed to text if needed
title = "Model Results",
dep.var.labels = c("Log of Sale Price"),
covariate.labels = c(
"Log(Total Livable Area)",
"Number of Bedrooms",
"Number of Bathrooms",
"Building Age",
"Interior Condition",
"Median Household Income",
"Family Household Share (Family HH / Total HH)",
"% Bachelor's Degree Holders",
"% Vacant Housing Units",
"% White Population",
"Economic Density (EconKDE)",
"Economic Density² (EconKDE²)",
"Tree Density (TreeKDE)",
"Mean 3-Nearest Neighbor Distance",
"Income Q2","Income Q3", "Income Q4"
),
omit.stat = c("f", "ser"),
digits = 3,
out = "regression.html")
set.seed(123)
k <- 10
folds <- cut(seq(1, nrow(for_model_building)), breaks = k, labels = FALSE)
rmse_cv_1 <- mae_cv_1 <- r2_cv_1 <- numeric(k)
for (i in 1:k) {
test_indices <- which(folds == i)
cv_train <- for_model_building[-test_indices, ]
cv_test <- for_model_building[test_indices, ]
#fit model
fit <- lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
+interior_condition+quality_grade, data = cv_train)
preds <- predict(fit, newdata = cv_test)
#calculate metrics
rmse_cv_1[i] <- sqrt(mean((cv_test$log_price - preds)^2, na.rm = TRUE))
mae_cv_1[i] <- mean(abs(cv_test$log_price - preds), na.rm = TRUE)
ss_res <- sum((cv_test$log_price - preds)^2, na.rm = TRUE)
ss_tot <- sum((cv_test$log_price - mean(cv_test$log_price))^2, na.rm = TRUE)
r2_cv_1[i] <- 1 - (ss_res / ss_tot)
}
print(data.frame(Fold = 1:k, RMSE = round(rmse_cv_1, 4),
MAE = round(mae_cv_1, 4), R2 = round(r2_cv_1, 4)))
rmse_cv_2 <- mae_cv_2 <- r2_cv_2 <- numeric(k)
for (i in 1:k) {
test_indices <- which(folds == i)
cv_train <- for_model_building[-test_indices, ]
cv_test <- for_model_building[test_indices, ]
#fit model
fit <- lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
+interior_condition+median_income+I(family_hh/total_hh)
+pct_bachelors+pct_vacant+pct_white+quality_grade, data = cv_train)
preds <- predict(fit, newdata = cv_test)
#calculate metrics
rmse_cv_2[i] <- sqrt(mean((cv_test$log_price - preds)^2, na.rm = TRUE))
mae_cv_2[i] <- mean(abs(cv_test$log_price - preds), na.rm = TRUE)
ss_res <- sum((cv_test$log_price - preds)^2, na.rm = TRUE)
ss_tot <- sum((cv_test$log_price - mean(cv_test$log_price))^2, na.rm = TRUE)
r2_cv_2[i] <- 1 - (ss_res / ss_tot)
}
print(data.frame(Fold = 1:k, RMSE = round(rmse_cv_2, 4),
MAE = round(mae_cv_2, 4), R2 = round(r2_cv_2, 4)))
rmse_cv_3 <- mae_cv_3 <- r2_cv_3 <- numeric(k)
for (i in 1:k) {
test_indices <- which(folds == i)
cv_train <- for_model_building[-test_indices, ]
cv_test <- for_model_building[test_indices, ]
#fit model
fit <- lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
+interior_condition+median_income+I(family_hh/total_hh)
+pct_bachelors+pct_vacant+pct_white+EconKDE+I(EconKDE^2)+TreeKDE
+mean_3nn_dist+quality_grade, data = cv_train)
preds <- predict(fit, newdata = cv_test)
#calculate metrics
rmse_cv_3[i] <- sqrt(mean((cv_test$log_price - preds)^2, na.rm = TRUE))
mae_cv_3[i] <- mean(abs(cv_test$log_price - preds), na.rm = TRUE)
ss_res <- sum((cv_test$log_price - preds)^2, na.rm = TRUE)
ss_tot <- sum((cv_test$log_price - mean(cv_test$log_price))^2, na.rm = TRUE)
r2_cv_3[i] <- 1 - (ss_res / ss_tot)
}
print(data.frame(Fold = 1:k, RMSE = round(rmse_cv_3, 4),
MAE = round(mae_cv_3, 4), R2 = round(r2_cv_3, 4)))
rmse_cv_4 <- mae_cv_4 <- r2_cv_4 <- numeric(k)
for (i in 1:k) {
test_indices <- which(folds == i)
cv_train <- for_model_building[-test_indices, ]
cv_test <- for_model_building[test_indices, ]
#fit model
fit <- lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
+interior_condition+median_income+I(family_hh/total_hh)
+pct_bachelors+pct_vacant+pct_white+EconKDE+I(EconKDE^2)+TreeKDE
+mean_3nn_dist+MHI_quantile*age, data = cv_train)
preds <- predict(fit, newdata = cv_test)
#calculate metrics
rmse_cv_4[i] <- sqrt(mean((cv_test$log_price - preds)^2, na.rm = TRUE))
mae_cv_4[i] <- mean(abs(cv_test$log_price - preds), na.rm = TRUE)
ss_res <- sum((cv_test$log_price - preds)^2, na.rm = TRUE)
ss_tot <- sum((cv_test$log_price - mean(cv_test$log_price))^2, na.rm = TRUE)
r2_cv_4[i] <- 1 - (ss_res / ss_tot)
}
print(data.frame(Fold = 1:k, RMSE = round(rmse_cv_4, 4),
MAE = round(mae_cv_4, 4), R2 = round(r2_cv_4, 4)))
cv_summary <- data.frame(
Model = c("Model 1", "Model 2", "Model 3", "Model 4"),
CV_RMSE = round(c(mean(rmse_cv_1), mean(rmse_cv_2), mean(rmse_cv_3), mean(rmse_cv_4)), 3),
CV_MAE = round(c(mean(mae_cv_1), mean(mae_cv_2), mean(mae_cv_3), mean(mae_cv_4)), 3),
CV_R2 = round(c(mean(r2_cv_1), mean(r2_cv_2), mean(r2_cv_3), mean(r2_cv_4)), 3)
)
stargazer(cv_summary,
type = "text",
title = "10-Fold Cross-Validation Results",
summary = FALSE,
rownames = FALSE)
stargazer(cv_summary,
type = "html",
title = "10-Fold Cross-Validation Results",
summary = FALSE,
rownames = FALSE,
out = "cv_results.html")
# Show detailed fold-by-fold performance for model 4
detailed_cv <- data.frame(
Fold = 1:k,
RMSE = round(rmse_cv_4, 4),
MAE = round(mae_cv_4, 4),
R2 = round(r2_cv_4, 4)
)
print(detailed_cv)
# Summary statistics across folds
cat("\nModel 4 CV Performance Summary:\n")
cat("RMSE: Mean =", round(mean(rmse_cv_4), 4), ", SD =", round(sd(rmse_cv_4), 4), "\n")
cat("MAE:  Mean =", round(mean(mae_cv_4), 4), ", SD =", round(sd(mae_cv_4), 4), "\n")
cat("R²:   Mean =", round(mean(r2_cv_4), 4), ", SD =", round(sd(r2_cv_4), 4), "\n")
#generate predictions using model4
final_predictions <- predict(model4, newdata = for_model_building)
#create scatter plot
plot(for_model_building$log_price, final_predictions,
xlab = "Actual Log(Sale Price)",
ylab = "Predicted Log(Sale Price)",
main = "Predicted vs. Actual Sale Prices",
pch = 16, col = rgb(0, 0, 1, 0.3))
abline(0, 1, col = "red", lwd = 2, lty = 2)
coefs <- coef(model4)[-1]
feature_importance <- data.frame(
Feature = names(coefs),
Coefficient = round(as.numeric(coefs), 4)
)
print(feature_importance)
#The most important feature is I(EconKDE^2). It has the largest coefficient compared for all the other features. This reflects that being close to commerical activities is really important to housing prices. There is also a pattern that spatial features are important.
#generate diagnostic plots
plot(model4)
#Residual Plot:
#The Residuals vs Fitted plot shows mild heteroskedasticity and a small degree of curvature, which is expected in a dataset of this size, and the model performs consistently well across most of the fitted range with only a few isolated outliers.
#QQ-Plot:
#The Q-Q plot shows slight deviation from normality in the extreme tails, which is common in large real-world housing datasets, while the majority of residuals align closely with the theoretical normal distribution, indicating that model inference remains reliable.
#identify influential observations
cooks_d <- cooks.distance(model4)
plot(cooks_d, type = "h", main = "Cook's Distance",
ylab = "Cook's Distance", xlab = "Observation")
abline(h = 4/nrow(for_model_building), col = "red", lty = 2)
#The Cook’s Distance plot shows that while a few observations exert relatively higher influence on the regression model, the vast majority of cases have negligible influence, and no single data point appears to unduly distort the model’s estimated coefficients.
